---
title: "playground"
author: "TJ Sipin"
date: "3/2/2022"
output: 
  html_document:
    code_folding: hide
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE)

library(knitr)
library(readr)
library(tidyverse)
library(dplyr)
library(gt)
library(ggplot2) 
library(dplyr)
library(tree)
library(gbm)
library(ISLR)
library(randomForest) 
library(MASS)
library(vip)
library(caret)
library(cluster)
library(tidymodels)
library(corrr)
library(discrim)
library(class)
library(ranger)
library(glmnet)
library(e1071)
```
Notes:
- 4 models
- How to deal with extremes? (aka create equal amounts of observations across 0-1) data augmentation? (cat)
- How to do CV?
- Lasso and ridge for variable selection

## Data processing
```{r}
koi = read.csv("cumulative.csv")
koi <- dplyr::select(koi,-c(kepoi_name, koi_teq_err1, koi_teq_err2, rowid, kepid, kepler_name, koi_tce_delivname))

# 2 level factor

# saving old koid.2 as koid.3

koi = na.omit(koi)

koid.2 = koi %>%
  mutate(level = as.factor(ifelse(koi_score <= median(koi_score), "Low", "High")))

koid.2b = koi %>%
  mutate(level = as.factor(ifelse(koi_score <= median(koi_score), "Low", "High")))

koid.3 = koi %>%
  mutate(level = as.factor(ifelse(koi_score <= .1, "Low",
                                  ifelse(koi_score <= .9, "Medium",
                                         ifelse(koi_score > .9, "High", 0)))))
koid.3 = koid.3 %>% dplyr::select(c(level, koi_disposition, koi_pdisposition, koi_score, koi_fpflag_nt, koi_fpflag_ss, koi_fpflag_co, koi_fpflag_ec, koi_period, koi_time0bk, koi_impact, koi_duration, koi_depth, koi_prad, koi_teq, koi_insol, koi_model_snr, koi_tce_plnt_num, koi_steff, koi_slogg, koi_srad, ra, dec, koi_kepmag))


# train and test split koid.3

train.3 = sample(1:nrow(koid.3), nrow(koid.3) * .8)
train.koid.3 = koid.3[train.3,]
test.koid.3 = koid.3[-train.3,]


# modifying koid.2
koid.2 = koid.2 %>% dplyr::select(c(level, koi_disposition, koi_pdisposition, koi_score, koi_fpflag_nt, koi_fpflag_ss, koi_fpflag_co, koi_fpflag_ec, koi_period, koi_time0bk, koi_impact, koi_duration, koi_depth, koi_prad, koi_teq, koi_insol, koi_model_snr, koi_tce_plnt_num, koi_steff, koi_slogg, koi_srad, ra, dec, koi_kepmag))

# train and test split

train = sample(1:nrow(koid.2), nrow(koid.2) * .8)
train.koid.2 = koid.2[train,]
test.koid.2 = koid.2[-train,]
```


## EDA

```{r}
# correlation
koid.2.corr <- koid.2 %>% dplyr::select(-c(koi_disposition, koi_pdisposition,level)) %>% correlate()

koid.2.corr

rplot(koid.2.corr)

# data exploration
summary(koid.2)

# graph of univariate, multivariate relationships between outcome and predictor(s) or between predictors

# histograms

hist(koid.2$koi_score, col = 10)

# make splits at 0.1 and 0.9 for koid.3?
```

## Linear regression
```{r}
# koid.2
lm_spec <- linear_reg() %>%
  set_mode("regression") %>%
  set_engine("lm")

lm_fit <- lm_spec %>%
  fit(koi_score ~ . - level, data = train.koid.2)

lm_fit

lm_fit %>% summary()

# normal prediction
predict(lm_fit, new_data = test.koid.2)

# confidence interval
predict(lm_fit, new_data = test.koid.2, type = "conf_int")

# evaluate performance of model
bind_cols(predict(lm_fit, new_data = test.koid.2),
          test.koid.2) %>%
  dplyr::select(koi_score, .pred)

summ_lm_fit <- summary(lm_fit)


# koid.3
```
Results:
- Model is pretty good
- Some negative values
- Extremes work well, nonextremes don't work too well

## Logistic regression 


```{r}
lr_spec <- logistic_reg() %>%
  set_engine("glm") %>%
  set_mode("classification")

lr_fit <- lr_spec %>%
  fit(level ~ . - koi_score, data = train.koid.2)

lr_fit
tidy(lr_fit)

predict(lr_fit, new_data = test.koid.2, type="prob")
augment(lr_fit, new_data = test.koid.2) %>%
  conf_mat(truth = level, estimate = .pred_class)

augment(lr_fit, new_data = test.koid.2) %>%
  conf_mat(truth = level, estimate = .pred_class) %>%
  autoplot(type="heatmap")

# accuracy

augment(lr_fit, new_data = test.koid.2) %>%
  accuracy(truth = level, estimate = .pred_class)

# which observations failed?

```

Results: 
- Incredibly high estimate accuracy

## LDA
```{r}
lda_spec <- discrim_linear() %>% 
  set_mode("classification") %>%
  set_engine("MASS")

lda_fit <- lda_spec %>% 
  fit(level ~ . - koi_score, data = train.koid.2)

lda_fit

predict(lda_fit, new_data = test.koid.2)
predict(lda_fit, new_data = test.koid.2, type="prob")

# performance
augment(lda_fit, new_data = test.koid.2) %>%
  conf_mat(truth = level, estimate = .pred_class)

augment(lda_fit, new_data = test.koid.2) %>%
  accuracy(truth = level, estimate = .pred_class)



#### koid.3

lda_spec <- discrim_linear() %>% 
  set_mode("classification") %>%
  set_engine("MASS")

lda_fit <- lda_spec %>% 
  fit(level ~ . - koi_score, data = train.koid.3)

lda_fit

predict(lda_fit, new_data = test.koid.3)
predict(lda_fit, new_data = test.koid.3, type="prob")

# performance
augment(lda_fit, new_data = test.koid.3) %>%
  conf_mat(truth = level, estimate = .pred_class)

augment(lda_fit, new_data = test.koid.3) %>%
  accuracy(truth = level, estimate = .pred_class)
```

## QDA *PROBLEMS*
```{r}
# qda_spec <- discrim_quad() %>%
#   set_mode("classification") %>%
#   set_engine("MASS")
# 
# 
# # Error: rank deficiency
# qda_fit <- qda_spec %>%
#   fit(level ~ . - koi_score, data = train.koid.2)
```

## Naive Bayes

```{r}
nb_spec <- naive_Bayes() %>%
  set_mode("classification") %>%
  set_engine("klaR") %>%
  set_args(usekernel = TRUE)

# Error: zero variance when usekernel = FALSE
# Warning: Numerical 0 probability for all observations
nb_fit <- nb_spec %>%
  fit(level ~ . - koi_score, data = train.koid.2)
augment(nb_fit, new_data = test.koid.2) %>%
  conf_mat(truth = level, estimate = .pred_class)

augment(nb_fit, new_data = test.koid.2) %>%
  accuracy(truth = level, estimate = .pred_class)


#### koid.3
nb_spec <- naive_Bayes() %>%
  set_mode("classification") %>%
  set_engine("klaR") %>%
  set_args(usekernel = TRUE)

# Error: zero variance when usekernel = FALSE
# Warning: Numerical 0 probability for all observations
nb_fit <- nb_spec %>%
  fit(level ~ . - koi_score, data = train.koid.3)
augment(nb_fit, new_data = test.koid.3) %>%
  conf_mat(truth = level, estimate = .pred_class)

augment(nb_fit, new_data = test.koid.3) %>%
  accuracy(truth = level, estimate = .pred_class)

```

## KNN

```{r}
knn_spec <- nearest_neighbor(neighbors = 3) %>% 
  set_mode("classification") %>%
  set_engine("kknn")

knn_fit <- knn_spec %>%
  fit(level ~ . - koi_score, data = train.koid.2)

knn_fit

# Performance

augment(knn_fit, new_data = test.koid.2) %>%
  conf_mat(truth = level, estimate = .pred_class)

augment(knn_fit, new_data = test.koid.2) %>%
  accuracy(truth = level, estimate = .pred_class)


#### koid.3
knn_spec <- nearest_neighbor(neighbors = 3) %>% 
  set_mode("classification") %>%
  set_engine("kknn")

knn_fit <- knn_spec %>%
  fit(level ~ . - koi_score, data = train.koid.3)

knn_fit

# Performance

augment(knn_fit, new_data = test.koid.3) %>%
  conf_mat(truth = level, estimate = .pred_class)

augment(knn_fit, new_data = test.koid.3) %>%
  accuracy(truth = level, estimate = .pred_class)
```

## k means clustering



```{r pressure, echo=FALSE}
# skoid = scale(koid.2, center=T, scale=T)
# km = kmeans(skoid, centers = 3)

```


## k-Fold CV

```{r}
# allK = 1:50
# 
# validation.error = rep(NA, 50)
# 
# do.chunk <- function(chunkid, folddef, Xdat, Ydat, ...){
#   train = (folddef != chunkid)
#   
#   Xtr = Xdat[train,]
#   ytr
# }


```

