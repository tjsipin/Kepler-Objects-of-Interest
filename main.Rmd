---
title: "Data Exploration"
author: "TJ Sipin"
date: "1/23/2022"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(knitr)
library(readr)
library(tidyverse)
library(dplyr)
library(gt)
library(ggplot2)
library(dplyr)
library(tree)
library(gbm)
library(ISLR)
library(randomForest) 
library(MASS)
library(vip)
library(caret)
```


## R Markdown

First, let's set up our data so we're creating a discrete response variable `Level`.

```{r cars}
koi = read.csv("cumulative.csv")
koi <- dplyr::select(koi,-c(kepoi_name, koi_teq_err1, koi_teq_err2, rowid, kepid, kepler_name, koi_tce_delivname))
summary(koi)
dim(koi)

# Creating discrete response variable
low_level = 1/3
med_level = 2/3
high_level = 1

koid = koi %>%
  mutate(level = as.factor(ifelse(koi_score <= low_level, "Low",
                           ifelse(koi_score <= med_level, "Medium",
                           ifelse(koi_score > med_level, "High", 0)))))

koid.2 = koi %>%
  mutate(level = as.factor(ifelse(koi_score <= median(koi_score), "Low", "High")))

# continuous outcome
```

## Subset 1: Removing NA values

For now: goal is to use a regression tree to analyze the Carseats data set. Now we split our data into training and test sets.

```{r pressure, echo=FALSE}
koid = na.omit(koid)
dim(koid)

koid.2 = na.omit(koid.2)

# SET SEED FOR REPRODUCIBILITY
set.seed(3)

# SAMPLE 70% OF OBSERVATIONS AS TRAINING DATA
train = sample(1:nrow(koid), nrow(koid)*0.7)
train.koid = koid[train,]

# THE REST ARE TEST DATA
test.koid = koid[-train,]

fill.na.with.mode = function(df){
    cols = colnames(df)
    for (col in cols){
        if(class(df[[col]])=='factor'){
            x = summary(df[[col]])
            mode = names(x[which.max(x)])
            df[[col]][is.na(df[[col]])]=mode
        }
        else{
            df[[col]][is.na(df[[col]])]=0
        }
    }
    return (df)
}
fill.na.with.mode(koid.2)

```

Now, we use 10-fold Cross Validation to select the best tree size, pruning the tree to the target number. Then we calculate the test error rate for future purposes.


```{r}
lda(level ~ . - koi_score, data = koid)
tree.koid = tree(level ~ . - koi_score,, data = koid, subset = train)
summary(tree.koid)

```

## Linear Regression

```{r}
lm.koi = lm(level ~ . - koi_score, data = koid)

prob.training.lm = predict(lm.koi, type="response")

koid = koid %>%
  mutate(PREDlevel.lm = as.factor(ifelse(prob.training.lm <= low_level, "Low",
                           ifelse(prob.training.lm <= med_level, "Medium",
                           ifelse(prob.training.lm > med_level, "High", 0)))))
error = table(pred = koid$PREDlevel.lm, true = koid$level)
error
```

## Logistic Regression

```{r}
glm.koi = glm(level ~ . - koi_score, data = koid, family = binomial)
summary(glm.koi)
# 
# glm.koid.2 = glm(level ~ . - koi_score, data = koid.2, family = binomial)

# Specify type="response" to get the estimated probabilities
prob.training = predict(glm.koi, type="response")

koid = koid %>%
  mutate(PREDlevel = as.factor(ifelse(prob.training <= low_level, "Low",
                           ifelse(prob.training <= med_level, "Medium",
                           ifelse(prob.training > med_level, "High", 0)))))

# Confusion matrix (training error/accuracy)
error = table(pred=koid$PREDlevel, true=koid$level)
error

# Test accuracy rate
sum(diag(error))/sum(error)

# Test error rate (Classification Error)
1 - sum(diag(error))/sum(error)


confusionMatrix(data = koid$PREDlevel, reference = koid$level, mode = "everything")
```

# Random forest
```{r}



```