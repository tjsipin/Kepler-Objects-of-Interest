---
title: "playground"
author: "TJ Sipin"
date: "3/2/2022"
output: 
  html_document:
    code_folding: hide

---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, 
                      cache = TRUE,
                      warning = FALSE,
                      message = FALSE)
options(scipen = 100)
library(knitr)
library(readr)
library(tidyverse)
library(dplyr)
#install.packages("gt")
#library(gt)
library(ggplot2)
library(dplyr)
library(tree)
library(gbm)
library(ISLR)
library(randomForest) 
library(MASS)
library(vip)
library(caret)
library(cluster)
library(tidymodels)
library(corrr)
library(discrim)
library(class)
library(gapminder)
library(kknn)
library(reshape2)
library(ggplot2)
library(rpart)
library(rpart.plot)
library(dendextend)
library(xgboost)
```
Notes:
- 4 models
- How to deal with extremes? (aka create equal amounts of observations across 0-1) data augmentation? (cat)
- How to do CV?
- Lasso and ridge for variable selection


## Data Wrangling and Cleaning


```{r}
koi = read.csv("cumulative.csv")
koi <- dplyr::select(koi,-c(kepoi_name, koi_teq_err1, koi_teq_err2, rowid, kepid, kepler_name, koi_tce_delivname))

# data wrangle koi

koi = na.omit(koi)
koi = koi %>% dplyr::select(c(koi_disposition, koi_fpflag_nt, koi_fpflag_ss, 
                              koi_fpflag_co, koi_fpflag_ec, koi_period, koi_time0bk, koi_impact, 
                              koi_duration, koi_depth, koi_prad, koi_teq, koi_insol, koi_model_snr, 
                              koi_steff, koi_slogg, koi_srad, ra, dec, koi_kepmag)) %>%
  mutate(koi_disposition = as.character(koi_disposition)) %>%
  mutate(koi_fpflag_nt = factor(koi_fpflag_nt)) %>%
  mutate(koi_fpflag_ss = factor(koi_fpflag_ss)) %>%
  mutate(koi_fpflag_co = factor(koi_fpflag_co)) %>%
  mutate(koi_fpflag_ec = factor(koi_fpflag_ec))
```

## Exploratory Data Analysis

```{r}
# correlation
koi.corr <- koi %>%
  dplyr::select(-c(koi_disposition, koi_fpflag_co, koi_fpflag_ec, koi_fpflag_nt, koi_fpflag_ss)) %>%
  correlate()

koi.corr

rplot(koi.corr)

# data exploration
summary(koi)

# graph of univariate, multivariate relationships between outcome and predictor(s) or between predictors

# histograms

ggplot(koi, aes(koi$koi_disposition)) + 
  geom_bar(stat = "count")

# qq plot?
```


## Splitting KOI Data into Training and Testing Sets

```{r}
# CV train and test split
koi_split <- initial_split(koi,
                           strata = koi_disposition)
koi_train <- training(koi_split)
koi_test <- testing(koi_split)
```

## Recipe

```{r}
# Recipe
recipe <- recipe(
  koi_disposition ~ koi_fpflag_nt + koi_fpflag_ss + koi_fpflag_co +
    koi_fpflag_ec + koi_period + koi_time0bk + koi_impact + koi_duration + koi_depth + koi_prad + 
    koi_teq + koi_insol + koi_model_snr + koi_steff + koi_slogg + koi_srad + 
    ra + dec + koi_kepmag, data = koi_train) %>%
  step_dummy(all_nominal(), -all_outcomes()) %>%
  step_normalize(all_predictors()) %>%
  step_zv(all_predictors()) %>%
  step_impute_median(all_predictors())
```


## Setting Up k-Fold Cross Validation

```{r}
koi_folds <- vfold_cv(koi_train)
save(koi_folds, recipe, file = "model_setup.rda")
```

# Models

## K-Nearest Neighbor
```{r}
# lab04

XTrain = koi_train %>% dplyr::select(-koi_disposition)
YTrain = koi_train$koi_disposition
XTest = koi_test %>% dplyr::select(-koi_disposition)
YTest = koi_test$koi_disposition

do.chunk <- function(chunkid, folddef, Xdat, Ydat, ...) {
  # Get training index
  train = (folddef!=chunkid)
  
  # Get training set by the above index
  Xtr = Xdat[train,]
  Ytr = Ydat[train]
  
  # Get validation set
  Xvl = Xdat[!train,]
  
  # Get responses in validation set
  Yvl = Ydat[!train]
  
  # Predict training labels``
  predYtr = knn(train=Xtr, test=Xtr, cl=Ytr, ...)
  
  # Predict validation labels
  predYvl = knn(train=Xtr, test=Xvl, cl=Ytr, ...)
  
  data.frame(fold = chunkid,
             train.error = mean(predYtr != Ytr), # Training error for each fold
             val.error = mean(predYvl != Yvl))
}
```

```{r, eval=FALSE}

# Set 3-fold CV

nfold = 3

set.seed(123)
folds = cut(1:nrow(koi_train), breaks=nfold, labels=F) %>% sample()
folds


# Set error.folds as a vector to save validation errors in future
error.folds = NULL

# Give possible number of nearest neighbors to be considered
allK = 1:50

set.seed(234)

# Loop through different number of neighbors
for(k in allK) {
  # Loop through different chunk id
  for (j in seq(3)) {
    tmp = do.chunk(chunkid = j, folddef = folds, Xdat = XTrain, Ydat = YTrain, k = k)
    
    tmp$neighbors = k # Records last number of neighbor
    
    error.folds = rbind(error.folds, tmp) # combines results
  }
}

save(error.folds, file = "error.rda")
```

```{r}
load("error.rda")

head(error.folds, 10)

# Transform the format of error.folds for further convenience
errors = melt(error.folds, id.vars = c('fold', 'neighbors'), value.name = 'error')

# Choose the number of neighbors which minimizes validation error
val.error.means = errors %>%
  # Select all rows of validation errors
  filter(variable=='val.error') %>%
  # Group the selected data frame by neighbors
  group_by(neighbors, variable) %>%
  # Calculate CV error rate for each k
  summarise_each(funs(mean), error) %>%
  # Remove existing group
  ungroup() %>%
  filter(error==min(error))

# Best num of neighbors (if tie, pick larger number of neighbors for simpler model)

bestneighbor = max(val.error.means$neighbors)
bestneighbor

set.seed(345)
pred.YTest = knn(train = XTrain, test = XTest, cl = YTrain, k = bestneighbor)

# Confusion matrix
conf.matrix = table(predicted = pred.YTest, true = YTest)

# Test error rate

print(paste("Test error rate: ", 1 - sum(diag(conf.matrix)/sum(conf.matrix))))

# Plot errors
ggplot(errors, aes(x = neighbors, y = error, color = variable)) + 
  geom_line(aes(group = interaction(variable,fold))) +
  stat_summary(aes(group = variable), fun = "mean", geom = "line", size = 3) +
  geom_vline(aes(xintercept = bestneighbor), linetype = "dashed")

```


## Decision Tree

```{r, eval = TRUE}
#creating the model
  
tree_model <- decision_tree() %>%
  set_engine("rpart") %>%
  set_mode("classification")

set.seed(1)
koi$koi_disposition <- as.factor(koi$koi_disposition)
cell_folds <- vfold_cv(koi_train, v = 5)

tree_wf <- workflow() %>%
  add_recipe(recipe)%>%
  add_model(tree_model)
  

# Tree Grid
tree_grid <- grid_regular(
  cost_complexity(),
  tree_depth(),
  levels = 5
)


tree_res <- tree_wf %>%
  tune_grid(
    resamples = cell_folds,
    grid = tree_grid
  )

param_grid <- grid_regular(cost_complexity(range = c(-3, -1)), levels = 10)

tune_res <- tune_grid(
  tree_wf, 
  resamples = cell_folds, 
  grid = param_grid, 
  metrics = metric_set(accuracy)
)


tree_spec <- decision_tree() %>%
  set_engine("rpart")

#classification tree

class_tree_spec <- tree_spec %>%
  set_mode("classification")
  
#making column into factor

koi_train$koi_disposition <-as.factor(koi_train$koi_disposition)

class_tree_fit <- class_tree_spec %>%
  fit(koi_disposition ~ ., data = koi_train)

class_tree_fit %>%
  extract_fit_engine() %>%
  rpart.plot()
  
#the training accuracy

accurate = augment(class_tree_fit, new_data = koi) %>%
  accuracy(truth = koi_disposition, estimate = .pred_class)
accurate
  
#the confusion matrix

confmat = augment(class_tree_fit, new_data = koi) %>%
  conf_mat(truth = koi_disposition, estimate = .pred_class)
confmat

#fitting the model on training data

class_tree_fit <- fit(class_tree_spec, koi_disposition ~ ., data = koi_train)

class_tree_wf <- workflow() %>%
  add_model(class_tree_spec %>% set_args(cost_complexity = tune())) %>%
  add_recipe(recipe)

koi_fold <- vfold_cv(koi_train)

param_grid <- grid_regular(cost_complexity(range = c(-3, -1)), levels = 10)

```

```{r, eval=FALSE}
tune_res <- tune_grid(
  class_tree_wf, 
  resamples = koi_fold, 
  grid = param_grid, 
  metrics = metric_set(accuracy))

save(tune_res, file = "tree_tune.rda")
```


```{r}
load("tree_tune.rda")

autoplot(tune_res)

best_complexity <- select_best(tune_res)

class_tree_final <- finalize_workflow(class_tree_wf, best_complexity)

koi_test$koi_disposition <- as.factor(koi_test$koi_disposition)

class_tree_final_fit <- fit(class_tree_final, data = koi_test)


class_tree_final_fit %>%
  extract_fit_engine() %>%
  rpart.plot()
  
koi_split <- initial_split(koi,
                           strata = koi_disposition)

final_tree_fit = class_tree_final %>% last_fit(koi_split)

final_tree_fit %>%
  collect_metrics()
```

## Boosted Tree


```{r, eval = FALSE}
# Creating model

boost_model <- boost_tree(min_n= tune(), 
                          mtry =tune(), 
                          learn_rate= tune(),
                          mode = "classification") %>%
  set_engine("xgboost")

boost_wflow <-
  workflow() %>%
  add_recipe(recipe) %>% 
  add_model(boost_model)

# Trying different parameters (mtry from 2 to 12, same as rf)

boost_params <- parameters(boost_model) %>%
  update(mtry = mtry(range = c(2, 12)))

# Define grid

boost_grid <- grid_regular(boost_params,
                           levels=3
)

# Tuning model

boost_tune <- boost_wflow %>%
  tune_grid(resamples = koi_folds,
            grid = boost_grid)

# Boost results for metrics

boost_res <- 
  boost_wflow %>% 
    tune_grid(resamples = koi_folds,
          grid = boost_grid,
          metrics = metric_set(
            recall, precision, f_meas,
            accuracy, kap,
            roc_auc, sens, spec
          ))

save(boost_tune, boost_res, file = "boost_tune.rda")
```

```{r}
load("boost_tune.rda")

autoplot(boost_tune)

boost_res %>% collect_metrics(summarize = TRUE)

```



## Random Forest

```{r, eval = TRUE}
rf_model <-
  rand_forest(min_n = tune(),
              mtry = tune(),
              mode = "classification") %>%
  set_engine("ranger")


rf_workflow <- workflow() %>%
  add_model(rf_model)  %>%
  add_recipe(recipe)

rf_params <- parameters(rf_model) %>%
  update(mtry = mtry(range = c(2, 12)))


rf_grid <- grid_regular(rf_params,
                        levels = 3)

```

```{r, eval = FALSE}

rf_res <- rf_workflow %>%
  tune_grid(resamples = koi_folds,
            grid = rf_grid)



rf_tune <- rf_workflow %>%
  tune_grid(resamples = koi_folds,
            grid = rf_grid)

save(rf_tune, rf_workflow, rf_res, file = "rf_tune.rda")
```

```{r}

load("rf_tune.rda")

autoplot(rf_tune)

show_best(rf_tune) %>% dplyr::select(-.estimator, -.config)

```

# Best Model 

Upon comparison among all four models, the random forest performed the best on the test data, but by only a margin of 0.001 compared to the boosted tree model.

We use https://rviews.rstudio.com/2019/06/19/a-gentle-intro-to-tidymodels/ as a guide to explore other metrics.

First, we make a final model by taking the best parameters obtained in the previous section and run the fit.

```{r}

koi_train$koi_disposition <- as.factor(koi_train$koi_disposition)
koi_test$koi_disposition <- as.factor(koi_test$koi_disposition)

final_model <- 
  rand_forest(min_n = 2,
              mtry = 7,
              mode = "classification") %>%
  set_engine("ranger") %>%
  fit(koi_disposition ~ ., data = koi_train)

final_model %>%
  predict(koi_test) %>%
  bind_cols(koi_test) %>%
  metrics(truth = koi_disposition, estimate = .pred_class)
```

Now we can explore the performance of our model by each predicted value in our sample space: `CANDIDATE`, `CONFIRMED`, and `FALSE POSITIVE`.

```{r}
# Per classifier metrics

koi_probs <- final_model %>%
  predict(koi_test, type = "prob") %>%
  bind_cols(koi_test)

# gain curve plot
koi_probs %>%
  gain_curve(koi_disposition, .pred_CANDIDATE:`.pred_FALSE POSITIVE`) %>%
  autoplot()

# if we target 25% of observations with highest probability of being CANDIDATE,
# we will get ~80% of all possible CANDIDATE observations

# roc curve plot
koi_probs %>%
  roc_curve(koi_disposition, .pred_CANDIDATE:`.pred_FALSE POSITIVE`) %>%
  autoplot()

# koi_disposition observations with FALSE POSITIVE have close to optimal model,
# CONFIRMED is good, and CANDIDATE is worst (still pretty good)
```

We ran both a gain curve plot and an ROC AUC plot. The ROC AUC plot is highly promising, with the `FALSE POSITIVE` plot is very close to the optimal plot. The plots for the other outcomes are also similarly good. Additionally, the gain curve plot tells us that for `CANDIDATE` and `CONFIRMED`, among the 25% of observations with highest probability of being `CANDIDATE` and `CONFIRMED` respectively, we will get ~75% of all`CANDIDATE` and `CONFIRMED` observations.

We now run different metrics and see the estimates for each.

```{r}
predict(final_model, koi_test, type = "prob") %>%
  bind_cols(predict(final_model, koi_test)) %>%
  bind_cols(dplyr::select(koi_test, koi_disposition)) %>%
  glimpse()

predict(final_model, koi_test, type = "prob") %>%
  bind_cols(predict(final_model, koi_test)) %>%
  bind_cols(dplyr::select(koi_test, koi_disposition)) %>%
  metrics(koi_disposition, .pred_CANDIDATE:`.pred_FALSE POSITIVE`, estimate = .pred_class)
```

